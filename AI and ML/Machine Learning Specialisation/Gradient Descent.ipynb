{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "858efcb6",
   "metadata": {},
   "source": [
    "# Components required for Gradient Descent\n",
    "\n",
    "- `Predictions of the model`\n",
    "\n",
    "- `Cost Computation Function for the predictions made by the model`\n",
    "\n",
    "- `Derivatives of the parameters of the model`\n",
    "\n",
    "All these components together comprise gradient descent which `finds the local minima for the parameters of the model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "566c40ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f847b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data\n",
    "x_train = np.array([1.0, 2.0])\n",
    "y_train = np.array([300.0, 500.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c636262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost Function an essential of gradient descent\n",
    "def compute_cost(x, y, w, b):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes the squared error of the model for any given parameters\n",
    "    Args:\n",
    "        x - Features\n",
    "        y - Targets\n",
    "        w, b - Model Parameters\n",
    "        \n",
    "    Returns:\n",
    "        total_cost - The cost of the model in predicting the output for any given parameters of the model (w, b)\n",
    "    \"\"\"\n",
    "    \n",
    "    # No of training samples\n",
    "    m = x.shape[0]\n",
    "    \n",
    "    # Cost Variable\n",
    "    total_cost = 0\n",
    "    \n",
    "    # Looping over all the training examples\n",
    "    for i in range(m):\n",
    "        \n",
    "        # Prediction\n",
    "        y_hat = w * x[i] + b\n",
    "        \n",
    "        # Error Calculation\n",
    "        del_error = (y_hat - y[i]) ** 2\n",
    "        \n",
    "        # Summation of delta \n",
    "        total_cost += del_error\n",
    "        \n",
    "    return (1 / (2 * m)) * total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cc1203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Gradient function is used to backtrack using gradient descent\n",
    "def compute_gradient(x, y, w, b):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes the gradient of the parameters used in the model to optimise their values\n",
    "    Args:\n",
    "        x - Features\n",
    "        y - Target\n",
    "        w, b - Model Parameters\n",
    "        \n",
    "    Returns:\n",
    "        dj_dw, dj_db the gradients of each of w and b after each training example\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tuned Parameters of the model\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    \n",
    "    # No of training examples\n",
    "    m = x.shape[0]\n",
    "    \n",
    "    for i in range(m):\n",
    "        \n",
    "        # Prediction\n",
    "        y_hat = w * x[i] + b\n",
    "        \n",
    "        # Gradients for the current training examples\n",
    "        dj_dw_i = (y_hat - y[i]) * x[i]\n",
    "        dj_db_i = y_hat - y[i]\n",
    "        \n",
    "        # Updating the gradients for the model\n",
    "        dj_dw += dj_dw_i\n",
    "        dj_db += dj_db_i\n",
    "        \n",
    "    # Scaling the values by taking the mean\n",
    "    dj_dw /= m\n",
    "    dj_db /= m\n",
    "    \n",
    "    return dj_dw, dj_db\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5d073cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Gradient Descent Function combines all the knowledge from before to complete the algorithm\n",
    "def gradient_descent(x, y, w_in, b_in, alpha, num_iterations, cost_function, gradient_function):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes the gradient descent and chooses the optimised values of w and b automatically\n",
    "    Args:\n",
    "        x - Features\n",
    "        y - Target\n",
    "        w_in, b_in - Initial values of the model parameters\n",
    "        num_iterations - No of times gradient descent is being run\n",
    "        cost_function - A helper function to find the cost of the each prediction\n",
    "        gradient_function - A helper function to find the gradient\n",
    "        \n",
    "    Returns:\n",
    "        w, b - Final value after tuning the parameters of the model\n",
    "        J_history - The list of all the values of J during gradient descent\n",
    "        P_history - The list of all the values of model parameters w, b during gradient descent\n",
    "    \"\"\"\n",
    "    \n",
    "    J_history, P_history = [], []\n",
    "    w, b = w_in, b_in\n",
    "    \n",
    "    # Using Gradient Descent for the given number of iterations\n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        # Computing the gradient\n",
    "        dj_dw, dj_db = gradient_function(x, y, w, b)\n",
    "        \n",
    "        # Updating the values of parameters\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "        \n",
    "        # Storing the values of w, b and J\n",
    "        if i < 10000:\n",
    "            \n",
    "            J_history.append(cost_function(x, y, w, b))\n",
    "            P_history.append([w, b])\n",
    "            \n",
    "        if i % math.ceil(num_iterations / 10) == 0:\n",
    "            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e} \",\n",
    "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
    "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    "            \n",
    "    return w, b, J_history, P_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69d8ee40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 7.93e+04  dj_dw: -6.500e+02, dj_db: -4.000e+02   w:  6.500e+00, b: 4.00000e+00\n",
      "Iteration 1000: Cost 3.41e+00  dj_dw: -3.712e-01, dj_db:  6.007e-01   w:  1.949e+02, b: 1.08228e+02\n",
      "Iteration 2000: Cost 7.93e-01  dj_dw: -1.789e-01, dj_db:  2.895e-01   w:  1.975e+02, b: 1.03966e+02\n",
      "Iteration 3000: Cost 1.84e-01  dj_dw: -8.625e-02, dj_db:  1.396e-01   w:  1.988e+02, b: 1.01912e+02\n",
      "Iteration 4000: Cost 4.28e-02  dj_dw: -4.158e-02, dj_db:  6.727e-02   w:  1.994e+02, b: 1.00922e+02\n",
      "Iteration 5000: Cost 9.95e-03  dj_dw: -2.004e-02, dj_db:  3.243e-02   w:  1.997e+02, b: 1.00444e+02\n",
      "Iteration 6000: Cost 2.31e-03  dj_dw: -9.660e-03, dj_db:  1.563e-02   w:  1.999e+02, b: 1.00214e+02\n",
      "Iteration 7000: Cost 5.37e-04  dj_dw: -4.657e-03, dj_db:  7.535e-03   w:  1.999e+02, b: 1.00103e+02\n",
      "Iteration 8000: Cost 1.25e-04  dj_dw: -2.245e-03, dj_db:  3.632e-03   w:  2.000e+02, b: 1.00050e+02\n",
      "Iteration 9000: Cost 2.90e-05  dj_dw: -1.082e-03, dj_db:  1.751e-03   w:  2.000e+02, b: 1.00024e+02\n",
      "(w,b) found by gradient descent: (199.9929,100.0116)\n"
     ]
    }
   ],
   "source": [
    "# Initialize parameters\n",
    "w_init = 0\n",
    "b_init = 0\n",
    "\n",
    "# Some gradient descent settings\n",
    "iterations = 10000\n",
    "tmp_alpha = 1.0e-2\n",
    "\n",
    "# Run gradient descent\n",
    "w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha, \n",
    "                                                    iterations, compute_cost, compute_gradient)\n",
    "\n",
    "# Result of gradient descent\n",
    "print(f\"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeebfc7e",
   "metadata": {},
   "source": [
    "## Predictions after tuning the parameters using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8927f2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 sqft house prediction 300.0 Thousand dollars\n",
      "1200 sqft house prediction 340.0 Thousand dollars\n",
      "2000 sqft house prediction 500.0 Thousand dollars\n"
     ]
    }
   ],
   "source": [
    "print(f\"1000 sqft house prediction {w_final*1.0 + b_final:0.1f} Thousand dollars\")\n",
    "print(f\"1200 sqft house prediction {w_final*1.2 + b_final:0.1f} Thousand dollars\")\n",
    "print(f\"2000 sqft house prediction {w_final*2.0 + b_final:0.1f} Thousand dollars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ae185f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
