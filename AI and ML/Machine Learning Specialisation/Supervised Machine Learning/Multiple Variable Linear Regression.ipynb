{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12356298",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n",
    "\n",
    "- Here the linear regression model calculates the prediction `y_hat` by taking `n features` into account\n",
    "\n",
    "- Each `ith` training example of the `m training examples` are accessed using `2 Dimensions [row, col]`\n",
    "\n",
    "- Model: f(`X`) = w[i] * x[i] + ..... + w[n] * x[n] + b\n",
    "\n",
    "- This model is run over `m training examples` to learn the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2293b781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import copy, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6165098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "X_train = np.array([\n",
    "    [2104, 5, 1, 45], \n",
    "    [1416, 3, 2, 40], \n",
    "    [852, 2, 1, 35]\n",
    "])\n",
    "y_train = np.array([460, 232, 178])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c5f8693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (3, 4), type: <class 'numpy.ndarray'>\n",
      "y_train.shape: (3,), type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Viewing the training data\n",
    "print(f\"X_train.shape: {X_train.shape}, type: {type(X_train)}\")\n",
    "print(f\"y_train.shape: {y_train.shape}, type: {type(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01eb8802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the intial values of the parameters\n",
    "w_in = np.array([0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "b_in = 785.1811367994083"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f44f0c",
   "metadata": {},
   "source": [
    "## Predicting a Value Without Vectoriasation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcddcb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the values without vectorisation\n",
    "def predict_single_loop(x, w, b):\n",
    "    \n",
    "    \"\"\"\n",
    "    Predicts the values of using the model without any vectorisation for a single example\n",
    "    Args:\n",
    "        x - Features\n",
    "        w, b - Parameters of the model\n",
    "        \n",
    "    Returns:\n",
    "        y_hat - Single prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    # No of training examples\n",
    "    n = x.shape[0]\n",
    "    y_hat = 0\n",
    "    \n",
    "    # Calculating the prediction using multiple values\n",
    "    for i in range(n):\n",
    "        p_i = w[i] * x[i]\n",
    "        y_hat += p_i\n",
    "    \n",
    "    # Adding the constant\n",
    "    y_hat += b\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bba481ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Row of Training Set:  [2104    5    1   45]\n",
      "The prediction: 459.9999976194083 \tActual Value: 460\n"
     ]
    }
   ],
   "source": [
    "# Predicting a single value using the model\n",
    "vec_x = X_train[0, :]\n",
    "y_hat = predict_single_loop(vec_x, w_in, b_in)\n",
    "\n",
    "print(\"Single Row of Training Set: \", vec_x)\n",
    "print(f\"The prediction: {y_hat} \\tActual Value: {y_train[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0938d380",
   "metadata": {},
   "source": [
    "## Predicting a value with vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bb34ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes the prediction of the model using vectorisation\n",
    "    Args:\n",
    "        x - Features\n",
    "        w, b - Parameters of the model\n",
    "        \n",
    "    Return:\n",
    "        y_hat - Scalar prediction for the single training example\n",
    "    \"\"\"\n",
    "    \n",
    "    # Here np.dot() multiplies and takes the sum of all the weights\n",
    "    y_hat = np.dot(x, w) + b\n",
    "    \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d25874d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Row of Training Set:  [2104    5    1   45]\n",
      "The prediction: 459.9999976194083 \tActual Value: 460\n"
     ]
    }
   ],
   "source": [
    "print(\"Single Row of Training Set: \", vec_x)\n",
    "print(f\"The prediction: {y_hat} \\tActual Value: {y_train[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1473e02c",
   "metadata": {},
   "source": [
    "## Computing the cost of the model for multiple variables\n",
    "\n",
    "- All the code below uses vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "857c95ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the cost of the model after multiple variable linear regression\n",
    "    Args:\n",
    "        X - Features\n",
    "        y - Target\n",
    "        w, b - Parameters of the model\n",
    "        \n",
    "    Returns:\n",
    "        cost - Scalar cost of the model after predicting m training examples\n",
    "    \"\"\"\n",
    "    \n",
    "    # No of training examples\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Cost\n",
    "    cost = 0\n",
    "    \n",
    "    # Looping to find the cost\n",
    "    for i in range(m):\n",
    "        \n",
    "        # Prediction\n",
    "        y_hat = np.dot(X[i], w) + b\n",
    "\n",
    "        # Error\n",
    "        err = (y_hat - y[i]) ** 2\n",
    "        \n",
    "        # Updating the cost\n",
    "        cost += err\n",
    "        \n",
    "    # Multiplying the constant\n",
    "    cost = (1 / (2 * m)) * cost\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a0a6c0",
   "metadata": {},
   "source": [
    "## Computing the cost of the prechosen parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1edb50f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost of the model is:  1.5578904045996674e-12\n"
     ]
    }
   ],
   "source": [
    "cost = compute_cost(X_train, y_train, w_in, b_in)\n",
    "print(\"The cost of the model is: \", cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c0ce48",
   "metadata": {},
   "source": [
    "## Computing the Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bb33e647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes the gradient for all the n features in the dataset\n",
    "    Args:\n",
    "        X - Feature\n",
    "        y - Targets\n",
    "        w, b - Parameters of the model\n",
    "        \n",
    "    Returns\n",
    "        dj_dw, dj_db - Gradients of all the parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # No of training examples and features\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Initial value of the gradients\n",
    "    dj_dw = np.zeros((n, ))\n",
    "    dj_db = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        \n",
    "        # Cost\n",
    "        error = (np.dot(w, X[i]) + b) - y[i]\n",
    "        \n",
    "        # Constant Gradient\n",
    "        dj_db += error\n",
    "        \n",
    "        # Looping to get gradient of all the features\n",
    "        for j in range(n):\n",
    "            dj_dw[j] += error * X[i, j]\n",
    "        \n",
    "    # Multiplying by constant\n",
    "    dj_dw /= m\n",
    "    dj_db /= m\n",
    "    \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bc700c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The First Gradient of dw is:  [-2.72623574e-03 -6.27197255e-06 -2.21745574e-06 -6.92403377e-05]\n",
      "The First Gradient of db is:  -1.6739251122999121e-06\n"
     ]
    }
   ],
   "source": [
    "# Checking the gradient for given weights\n",
    "temp_dw, temp_db = compute_gradient(X_train, y_train, w_in, b_in)\n",
    "\n",
    "print(\"The First Gradient of dw is: \", temp_dw)\n",
    "print(\"The First Gradient of db is: \", temp_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981cf6f8",
   "metadata": {},
   "source": [
    "## Computing the Gradient Descent of Multiple Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "60a64d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iterations):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes the gradient descent of multiple variables to optimise the parameters of the model\n",
    "    Args:\n",
    "        X - Features\n",
    "        y - Target\n",
    "        w, b - Parameters of the model\n",
    "        cost_function - Function which returns the cost of all the training examples\n",
    "        gradient_function - Function which returns the gradient of all the training examples for n features\n",
    "        alpha - Learning rate\n",
    "        num_iterations - No of times gradient descent is being run\n",
    "        \n",
    "    Returns:\n",
    "        w, b - Optimised parameters of the model\n",
    "    \"\"\"\n",
    "    \n",
    "    # History of cost during gradient descent\n",
    "    J_history = []\n",
    "    \n",
    "    # Avoid Modifying the global variable\n",
    "    w = copy.deepcopy(w_in)\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        # Calculating the gradients\n",
    "        dj_dw, dj_db = gradient_function(X, y, w, b)\n",
    "        \n",
    "        # Updating the weights\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "        \n",
    "        # Storing the cost values\n",
    "        if i < 10000:\n",
    "            J_history.append(cost_function(X, y, w, b))\n",
    "            \n",
    "        if i % math.ceil(num_iterations / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
    "            \n",
    "    return w, b, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9bcc58",
   "metadata": {},
   "source": [
    "## Using gradient descent to find the optimal values of the parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "38b137bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost  2529.46   \n",
      "Iteration  100: Cost   695.99   \n",
      "Iteration  200: Cost   694.92   \n",
      "Iteration  300: Cost   693.86   \n",
      "Iteration  400: Cost   692.81   \n",
      "Iteration  500: Cost   691.77   \n",
      "Iteration  600: Cost   690.73   \n",
      "Iteration  700: Cost   689.71   \n",
      "Iteration  800: Cost   688.70   \n",
      "Iteration  900: Cost   687.69   \n",
      "b,w found by gradient descent: -0.00,[ 0.20396569  0.00374919 -0.0112487  -0.0658614 ] \n"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "initial_w = np.zeros_like(w_in)\n",
    "initial_b = 0.\n",
    "\n",
    "# some gradient descent settings\n",
    "iterations = 1000\n",
    "alpha = 5.0e-7\n",
    "\n",
    "# run gradient descent \n",
    "w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,\n",
    "                                                    compute_cost, compute_gradient, \n",
    "                                                    alpha, iterations)\n",
    "\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348bfd9a",
   "metadata": {},
   "source": [
    "## Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9c177f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: 426.19, target value: 460\n",
      "prediction: 286.17, target value: 232\n",
      "prediction: 171.47, target value: 178\n"
     ]
    }
   ],
   "source": [
    "# Making Predictions\n",
    "m = X_train.shape[0]\n",
    "for i in range(m):\n",
    "    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986364de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
