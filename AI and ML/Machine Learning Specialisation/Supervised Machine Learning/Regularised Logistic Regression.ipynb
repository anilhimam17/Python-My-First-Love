{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55ec3099",
   "metadata": {},
   "source": [
    "# Overfitting\n",
    "\n",
    "### Overfitting is the process of training a model to perfectly predict all the data points in a training dataset. It is seen in different cases while training a model. Some of the cases of over fitting are as follows:\n",
    "\n",
    "- When there is limited data in the training set\n",
    "- When the model function is a higher order polynomial \n",
    "- When the weights of the model function vary in range and hence feature scaling\n",
    "\n",
    "# Dealing with Overfitting\n",
    "\n",
    "### An overfit model can be dealth with in many ways\n",
    "\n",
    "### Basic Measures (Try First)\n",
    "\n",
    "- Addition of more training examples in the training dataset\n",
    "- Excluding any unwanted features in the training dataset if necessary\n",
    "- Reducing the a higher order model to a simpler model\n",
    "\n",
    "### Regularization (If nothing works)\n",
    "\n",
    "- Regularization is the process of nullifying the effects of an overfit model by penalising the model for every wrong prediction made\n",
    "\n",
    "- This is done by introducing a new term to the cost function of logistic regression then `Regularization Term`\n",
    "\n",
    "- (lambda / 2m) * sum(w[j] ** 2)\n",
    "\n",
    "- Where `lambda` is the `Regularization Parameter`\n",
    "\n",
    "### Regularization thus effectively reduces the parameters of the model as a whole to a small range while keeping the cost function at a global minima\n",
    "\n",
    "- `Cost Function`: Same as Usual + `Regularization Term`\n",
    "\n",
    "- `Gradient dj_dw`: Same as Usual + `(lamdba / m) * w[j]`\n",
    "\n",
    "- `Gradient dj_db`: Same as Usual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8ed1eb",
   "metadata": {},
   "source": [
    "## Creating the model as before with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51c87f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary Imports\n",
    "import math, copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8e0453",
   "metadata": {},
   "source": [
    "## Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab2d2193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_val(x):\n",
    "    return 1 / (1 + math.e ** -(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d497176b",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2963ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
    "y_train = np.array([0, 0, 0, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d6f43",
   "metadata": {},
   "source": [
    "## Regularized Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59031d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_logistic(X, y, w, b, lambda_):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function calculates the simplified loss of every training example in logistic regression using \n",
    "    the above equation\n",
    "    \n",
    "    Args:\n",
    "        X - Features\n",
    "        y - Targets\n",
    "        w, b - Parameters of the model\n",
    "        lambda_ - Regularization Parameter\n",
    "        \n",
    "    Returns:\n",
    "        total_cost - A scalar value of the total cost of the model after iterating over all the training examples\n",
    "    \"\"\"\n",
    "    \n",
    "    # Size of the training data\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Total cost value\n",
    "    total_cost = 0\n",
    "    \n",
    "    # Regularization Term\n",
    "    reg_term = 0\n",
    "    \n",
    "    # Iterating over each training example to calculate the cost\n",
    "    for i in range(m):\n",
    "        \n",
    "        # Calculating the Linear Regression Value\n",
    "        z_wb = np.dot(X[i], w) + b\n",
    "        \n",
    "        # Mapping the prediction to Sigmoid Function\n",
    "        f_wb = sigmoid_val(z_wb)\n",
    "        \n",
    "        # Calculating the loss\n",
    "        total_cost += (-y[i] * math.log(f_wb)) - (1 - y[i]) * math.log(1 - f_wb)\n",
    "        \n",
    "    \n",
    "    # Calculating the Regularization Term\n",
    "    for j in range(n):\n",
    "        reg_term += w[j] ** 2\n",
    "        \n",
    "    reg_term *= (lambda_ / (2 * m))\n",
    "        \n",
    "    # Taking the mean of all the values and adding the reg term\n",
    "    total_cost /= m\n",
    "    total_cost += reg_term\n",
    "    \n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aed88b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost of the model is: 0.5335334530721841\n"
     ]
    }
   ],
   "source": [
    "# Checking the performance with temporary values\n",
    "w_temp = np.array([1, 1])\n",
    "b_temp = -3\n",
    "\n",
    "print(f\"The cost of the model is: {compute_cost_logistic(X_train, y_train, w_temp, b_temp, 1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc1d177",
   "metadata": {},
   "source": [
    "## Calculating the Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36f76ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_logistic(X, y, w, b, lambda_):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function computes the gradient of required by Gradient Descent to update the model parameters\n",
    "    \n",
    "    Args:\n",
    "        X - Features\n",
    "        y - Targets\n",
    "        w, b - Model Parameters\n",
    "        lambda_ - Regularization Parameter\n",
    "        \n",
    "    Returns:\n",
    "        dj_dw, dj_db - The optimized values of gradients for all the parameters for all the training examples for \n",
    "                       a given iteration of gradient descent\n",
    "    \"\"\"\n",
    "    \n",
    "    # Shape of the training date\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Initialisng the gradient values\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0\n",
    "    \n",
    "    # Looping over all the traning examples to calculate the gradients\n",
    "    for i in range(m):\n",
    "        \n",
    "        # Linear Regression\n",
    "        z_wb = np.dot(X[i], w) + b\n",
    "        \n",
    "        # Logistic Regression\n",
    "        f_wb = sigmoid_val(z_wb)\n",
    "        \n",
    "        # Calculating the error\n",
    "        err = f_wb - y[i]\n",
    "        \n",
    "        # Updating the gradients of each feature\n",
    "        for j in range(n):\n",
    "            dj_dw[j] += (err * X[i, j]) + ((lambda_ / m) * w[j])\n",
    "            \n",
    "        dj_db += err\n",
    "        \n",
    "    # Taking the mean of the gradients\n",
    "    dj_dw /= m\n",
    "    dj_db /= m\n",
    "        \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b049be70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db: 0.49861806546328574\n",
      "dj_dw: [0.8316667266120293, 0.9988394298399669]\n"
     ]
    }
   ],
   "source": [
    "# Computing the gradient for gradient descent\n",
    "w_tmp = np.array([2.,3.])\n",
    "b_tmp = 1.\n",
    "dj_dw_tmp, dj_db_tmp = compute_gradient_logistic(X_train, y_train, w_tmp, b_tmp, 1)\n",
    "\n",
    "print(f\"dj_db: {dj_db_tmp}\" )\n",
    "print(f\"dj_dw: {dj_dw_tmp.tolist()}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2480921",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2585a074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, alpha, lambda_, cost_function, gradient_function, num_iterations):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function computes the gradient descent for logistic regression and optimizes the parameters of the model \n",
    "    while finding a global minima to minimize the cost of the prediction made by the model\n",
    "    \n",
    "    Args:\n",
    "        X - Features\n",
    "        y - Targets\n",
    "        w, b - Parameters of the model\n",
    "        alpha - Learning Rate of the model\n",
    "        lambda_ - Regularization Parameter\n",
    "        cost_function - Used to calculate the cost of the model perdiction for each training example\n",
    "        gradient_function - Used to calculate the gradient of all the parameters used by the model\n",
    "        num_iterations - The no of times gradient descent algorithm is run on the given dataset\n",
    "        \n",
    "    Returns:\n",
    "        w, b - Final value of the parameters of the model after gradient descent\n",
    "    \"\"\"\n",
    "    \n",
    "    # Size of traning data\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Making a deep copy of the parameters to not affect the global variables\n",
    "    w = copy.deepcopy(w_in)\n",
    "    b = b_in\n",
    "    \n",
    "    # Storing the history of model cost during gradient descent\n",
    "    J_history = []\n",
    "    \n",
    "    # Looping over the training data for num_iterations\n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        # Compute Gradients\n",
    "        dj_dw, dj_db = gradient_function(X, y, w, b, lambda_)\n",
    "        \n",
    "        # Updating the parameters\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "        \n",
    "        # Storing the cost of every iteration\n",
    "        if i < 10000:\n",
    "            J_history.append(cost_function(X, y, w, b, lambda_))\n",
    "            \n",
    "        # Printing the cost for every 1000 iterations\n",
    "        if i % math.ceil(num_iterations / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]}   \")\n",
    "        \n",
    "    return w, b, J_history "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e40a747",
   "metadata": {},
   "source": [
    "## Optimizing the model after Regularized Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9458c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 0.6846857000420554   \n",
      "Iteration 1000: Cost 0.5203321414569039   \n",
      "Iteration 2000: Cost 0.5203016151380848   \n",
      "Iteration 3000: Cost 0.5203016077007424   \n",
      "Iteration 4000: Cost 0.5203016076989254   \n",
      "Iteration 5000: Cost 0.520301607698925   \n",
      "Iteration 6000: Cost 0.5203016076989251   \n",
      "Iteration 7000: Cost 0.520301607698925   \n",
      "Iteration 8000: Cost 0.520301607698925   \n",
      "Iteration 9000: Cost 0.520301607698925   \n",
      "\n",
      "updated parameters: w:[0.90411532 0.73588062], b:-2.3337541849928076\n"
     ]
    }
   ],
   "source": [
    "# Some Initial Values of Parameters of the model\n",
    "w_tmp  = np.zeros_like(X_train[0])\n",
    "b_tmp  = 0.\n",
    "\n",
    "# Gradient Descent Parameters\n",
    "alph = 0.1\n",
    "iters = 10000\n",
    "\n",
    "# Computing Gradient Descent\n",
    "w_out, b_out, _ = gradient_descent(X_train, y_train, w_tmp, b_tmp, alph, 1, compute_cost_logistic, \n",
    "                                   compute_gradient_logistic, iters) \n",
    "print(f\"\\nupdated parameters: w:{w_out}, b:{b_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b470f104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction: 0.3331975531912616 0 \t Actual Value: 0\n",
      "Model Prediction: 0.720357915618094 1 \t Actual Value: 1\n"
     ]
    }
   ],
   "source": [
    "# Predicting values using the model\n",
    "X_pred_1 = np.array([1, 1])\n",
    "y_pred_1 = 0\n",
    "\n",
    "X_pred_2 = np.array([2, 2])\n",
    "y_pred_2 = 1\n",
    "\n",
    "y_hat_1 = sigmoid_val(np.dot(w_out, X_pred_1) + b_out)\n",
    "y_hat_2 = sigmoid_val(np.dot(w_out, X_pred_2) + b_out)\n",
    "\n",
    "if y_hat_1 < 0.5:\n",
    "    print(f\"Model Prediction: {y_hat_1} 0 \\t Actual Value: {y_pred_1}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Model Prediction: {y_hat_1} 1 \\t Actual Value: {y_pred_1}\")\n",
    "    \n",
    "if y_hat_2 < 0.5:\n",
    "    print(f\"Model Prediction: {y_hat_2} 0 \\t Actual Value: {y_pred_2}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Model Prediction: {y_hat_2} 1 \\t Actual Value: {y_pred_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cee64b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
