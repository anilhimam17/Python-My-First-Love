{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af2d4153",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n",
    "\n",
    "- Here the linear regression model calculates the prediction `y_hat` by taking `n features` into account\n",
    "\n",
    "- Each `ith` training example of the `m training examples` are accessed using `2 Dimensions [row, col]`\n",
    "\n",
    "- Model: f(`X`) = w[i] * x[i] + ..... + w[n] * x[n] + b\n",
    "\n",
    "- This model is run over `m training examples` to learn the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "697b562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import copy, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8e88075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "X_train = np.array([\n",
    "    [2104, 5, 1, 45], \n",
    "    [1416, 3, 2, 40], \n",
    "    [852, 2, 1, 35]\n",
    "])\n",
    "y_train = np.array([460, 232, 178])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05bfc525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (3, 4), type: <class 'numpy.ndarray'>\n",
      "y_train.shape: (3,), type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Viewing the training data\n",
    "print(f\"X_train.shape: {X_train.shape}, type: {type(X_train)}\")\n",
    "print(f\"y_train.shape: {y_train.shape}, type: {type(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67aa8a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the intial values of the parameters\n",
    "w_in = np.array([0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "b_in = 785.1811367994083"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8051ad39",
   "metadata": {},
   "source": [
    "## Predicting a Value Without Vectoriasation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04a61019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the values without vectorisation\n",
    "def predict_single_loop(x, w, b):\n",
    "    \n",
    "    \"\"\"\n",
    "    Predicts the values of using the model without any vectorisation for a single example\n",
    "    Args:\n",
    "        x - Features\n",
    "        w, b - Parameters of the model\n",
    "        \n",
    "    Returns:\n",
    "        y_hat - Single prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    # No of training examples\n",
    "    n = x.shape[0]\n",
    "    y_hat = 0\n",
    "    \n",
    "    # Calculating the prediction using multiple values\n",
    "    for i in range(n):\n",
    "        p_i = w[i] * x[i]\n",
    "        y_hat += p_i\n",
    "    \n",
    "    # Adding the constant\n",
    "    y_hat += b\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fdf25a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Row of Training Set:  [2104    5    1   45]\n",
      "The prediction: 459.9999976194083 \tActual Value: 460\n"
     ]
    }
   ],
   "source": [
    "# Predicting a single value using the model\n",
    "vec_x = X_train[0, :]\n",
    "y_hat = predict_single_loop(vec_x, w_in, b_in)\n",
    "\n",
    "print(\"Single Row of Training Set: \", vec_x)\n",
    "print(f\"The prediction: {y_hat} \\tActual Value: {y_train[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f604bd66",
   "metadata": {},
   "source": [
    "## Predicting a value with vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f910f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes the prediction of the model using vectorisation\n",
    "    Args:\n",
    "        x - Features\n",
    "        w, b - Parameters of the model\n",
    "        \n",
    "    Return:\n",
    "        y_hat - Scalar prediction for the single training example\n",
    "    \"\"\"\n",
    "    \n",
    "    # Here np.dot() multiplies and takes the sum of all the weights\n",
    "    y_hat = np.dot(x, w) + b\n",
    "    \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cff4d1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Row of Training Set:  [2104    5    1   45]\n",
      "The prediction: 459.9999976194083 \tActual Value: 460\n"
     ]
    }
   ],
   "source": [
    "print(\"Single Row of Training Set: \", vec_x)\n",
    "print(f\"The prediction: {y_hat} \\tActual Value: {y_train[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd7e917",
   "metadata": {},
   "source": [
    "## Computing the cost of the model for multiple variables\n",
    "\n",
    "- All the code below uses vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9f15225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the cost of the model after multiple variable linear regression\n",
    "    Args:\n",
    "        X - Features\n",
    "        y - Target\n",
    "        w, b - Parameters of the model\n",
    "        \n",
    "    Returns:\n",
    "        cost - Scalar cost of the model after predicting m training examples\n",
    "    \"\"\"\n",
    "    \n",
    "    # No of training examples\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Cost\n",
    "    cost = 0\n",
    "    \n",
    "    # Looping to find the cost\n",
    "    for i in range(m):\n",
    "        \n",
    "        # Prediction\n",
    "        y_hat = np.dot(X[i], w) + b\n",
    "\n",
    "        # Error\n",
    "        err = (y_hat - y[i]) ** 2\n",
    "        \n",
    "        # Updating the cost\n",
    "        cost += err\n",
    "        \n",
    "    # Multiplying the constant\n",
    "    cost = (1 / (2 * m)) * cost\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b2836b",
   "metadata": {},
   "source": [
    "## Computing the cost of the prechosen parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cfb801a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost of the model is:  1.5578904045996674e-12\n"
     ]
    }
   ],
   "source": [
    "cost = compute_cost(X_train, y_train, w_in, b_in)\n",
    "print(\"The cost of the model is: \", cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6aaca5",
   "metadata": {},
   "source": [
    "## Computing the Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3c525462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes the gradient for all the n features in the dataset\n",
    "    Args:\n",
    "        X - Feature\n",
    "        y - Targets\n",
    "        w, b - Parameters of the model\n",
    "        \n",
    "    Returns\n",
    "        dj_dw, dj_db - Gradients of all the parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # No of training examples and features\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Initial value of the gradients\n",
    "    dj_dw = np.zeros((n, ))\n",
    "    dj_db = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        \n",
    "        # Cost\n",
    "        error = (np.dot(w, X[i]) + b) - y[i]\n",
    "        \n",
    "        # Constant Gradient\n",
    "        dj_db += error\n",
    "        \n",
    "        # Looping to get gradient of all the features\n",
    "        for j in range(n):\n",
    "            dj_dw[j] += error * X[i, j]\n",
    "        \n",
    "    # Multiplying by constant\n",
    "    dj_dw /= m\n",
    "    dj_db /= m\n",
    "    \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "67496047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The First Gradient of dw is:  [-2.72623574e-03 -6.27197255e-06 -2.21745574e-06 -6.92403377e-05]\n",
      "The First Gradient of db is:  -1.6739251122999121e-06\n"
     ]
    }
   ],
   "source": [
    "# Checking the gradient for given weights\n",
    "temp_dw, temp_db = compute_gradient(X_train, y_train, w_in, b_in)\n",
    "\n",
    "print(\"The First Gradient of dw is: \", temp_dw)\n",
    "print(\"The First Gradient of db is: \", temp_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bf66c7",
   "metadata": {},
   "source": [
    "## Computing the Gradient Descent of Multiple Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5858eca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iterations):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes the gradient descent of multiple variables to optimise the parameters of the model\n",
    "    Args:\n",
    "        X - Features\n",
    "        y - Target\n",
    "        w, b - Parameters of the model\n",
    "        cost_function - Function which returns the cost of all the training examples\n",
    "        gradient_function - Function which returns the gradient of all the training examples for n features\n",
    "        alpha - Learning rate\n",
    "        num_iterations - No of times gradient descent is being run\n",
    "        \n",
    "    Returns:\n",
    "        w, b - Optimised parameters of the model\n",
    "    \"\"\"\n",
    "    \n",
    "    # History of cost during gradient descent\n",
    "    J_history = []\n",
    "    \n",
    "    # Avoid Modifying the global variable\n",
    "    w = copy.deepcopy(w_in)\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        # Calculating the gradients\n",
    "        dj_dw, dj_db = gradient_function(X, y, w, b)\n",
    "        \n",
    "        # Updating the weights\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "        \n",
    "        # Storing the cost values\n",
    "        if i < 10000:\n",
    "            J_history.append(cost_function(X, y, w, b))\n",
    "            \n",
    "        if i % math.ceil(num_iterations / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
    "            \n",
    "    return w, b, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b116225",
   "metadata": {},
   "source": [
    "## Using gradient descent to find the optimal values of the parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2c71a4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost  2529.46   \n",
      "Iteration  100: Cost   695.99   \n",
      "Iteration  200: Cost   694.92   \n",
      "Iteration  300: Cost   693.86   \n",
      "Iteration  400: Cost   692.81   \n",
      "Iteration  500: Cost   691.77   \n",
      "Iteration  600: Cost   690.73   \n",
      "Iteration  700: Cost   689.71   \n",
      "Iteration  800: Cost   688.70   \n",
      "Iteration  900: Cost   687.69   \n",
      "b,w found by gradient descent: -0.00,[ 0.20396569  0.00374919 -0.0112487  -0.0658614 ] \n"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "initial_w = np.zeros_like(w_in)\n",
    "initial_b = 0.\n",
    "\n",
    "# some gradient descent settings\n",
    "iterations = 1000\n",
    "alpha = 5.0e-7\n",
    "\n",
    "# run gradient descent \n",
    "w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,\n",
    "                                                    compute_cost, compute_gradient, \n",
    "                                                    alpha, iterations)\n",
    "\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836b9b44",
   "metadata": {},
   "source": [
    "## Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "37a4470b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: 426.19, target value: 460\n",
      "prediction: 286.17, target value: 232\n",
      "prediction: 171.47, target value: 178\n"
     ]
    }
   ],
   "source": [
    "# Making Predictions\n",
    "m = X_train.shape[0]\n",
    "for i in range(m):\n",
    "    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d61b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
